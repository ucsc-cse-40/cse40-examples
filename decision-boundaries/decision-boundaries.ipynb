{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f6049f-899e-4912-b2f1-fe74d8400cec",
   "metadata": {},
   "source": [
    "# Decision Boundaries\n",
    "\n",
    "Once we have designed and trained a classifier, how do we analyze or debug it?\n",
    "Of course, we can look over metrics it produces (like F1 or RMSE), but those are summaries/aggregates of it's performance.\n",
    "They can tell you a lot, but sometimes you just need to visually see what your classifier is doing.\n",
    "\n",
    "One way we can visualize a classifier's performance is by looking at it's [decision boundary](https://en.wikipedia.org/wiki/Decision_boundary) (also called a *decision surface*).\n",
    "The decision boundary is boundary that a classifier uses in feature space to separate data points into different labels.\n",
    "This is most easily conceptualized with a linear decision boundary with a binary classifier.\n",
    "\n",
    "TODO: IMAGE\n",
    "\n",
    "TODO: DESCRIBE IMAGE\n",
    "\n",
    "feature space\n",
    "\n",
    "We generally only use two dimensions, but you can try using three.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "But we can also look at decision boundaries for classifiers with more than two labels.\n",
    "\n",
    "<center><img src=\"ternary-decision-boundary.png\" /></center>\n",
    "<center style='font-size: small'>Image from <a href='https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html'>scikit-learn</a>.</center>\n",
    "\n",
    "sklearn\n",
    "iris dataset with three labels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Decision boundaries do not have to be actual lines with a known formula.\n",
    "They are more just representations of what our classifier will tend to do.\n",
    "For example, we can visualize decision boundaries for an algorithm like KNN where no actual lines/curves are estimated.\n",
    "\n",
    "TODO: IMAGE\n",
    "\n",
    "<center><img src=\"clustering-decision-boundary.png\" /></center>\n",
    "<center style='font-size: small'>Image from <a href='https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html'>scikit-learn</a>.</center>\n",
    "\n",
    "(Note that the decision boundary will look a bit strange in some places because it is an approximation.)\n",
    "\n",
    "\n",
    "\n",
    "Now with the theory out of the way, let's actually work with some decision boundaries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO\n",
    "\n",
    "Classification\n",
    "\n",
    "Binary\n",
    "\n",
    "Decision Boundary\n",
    "\n",
    "Multiclass\n",
    "\n",
    "linear\n",
    " - kernel\n",
    "\n",
    "Untrained\n",
    "\n",
    "Improve Per Iteration\n",
    "\n",
    "Improve Per Data\n",
    "\n",
    "Non-linear\n",
    " - Good\n",
    " - Bad\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f461f-f2f9-407d-b223-d1589ab722cd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's make a function to help us plot a decision boundary given a classifier, features, and labels.\n",
    "You may recognize a similar function to this in HO4.\n",
    "This function will take in a trained classifier and train/test data.\n",
    "It will plot the classifier's decision boundary using [sklearn.inspection.DecisionBoundaryDisplay()](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) function,\n",
    "the training data with white outlines,\n",
    "and the test data with black outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e02a04-9283-4298-8e66-3b5a820bc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import sklearn.datasets\n",
    "import sklearn.svm\n",
    "\n",
    "FIGURE_SIZE = 5\n",
    "FIGURE_RESOLUTION = 500\n",
    "\n",
    "def visualize_decision_boundary(classifier,\n",
    "                                train_features, train_labels,\n",
    "                                test_features, test_labels,\n",
    "                                title = None):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary of a trained binary classifier\n",
    "    using the FIRST TWO columns of the passed in features.\n",
    "\n",
    "    The train data will be plotted in a lighter shade than the background with a white outline.\n",
    "    The test data will be plotted in a darker shade than the background with a black outline.\n",
    "    \"\"\"\n",
    "\n",
    "    figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (FIGURE_SIZE, FIGURE_SIZE))\n",
    "                                    \n",
    "    matplotlib.pyplot.suptitle(title)\n",
    "\n",
    "    # Score the classifier.\n",
    "    train_accuracy = classifier.score(train_features, train_labels)\n",
    "    test_accuracy = classifier.score(test_features, test_labels)\n",
    "    matplotlib.pyplot.title(\"Train Accuracy: %3.2f, Test Accuracy: %3.2f\" % (train_accuracy, test_accuracy))\n",
    "\n",
    "    all_features = pandas.concat([train_features, test_features])\n",
    "                                    \n",
    "    # Draw the decision boundary.\n",
    "    decision_boundary = sklearn.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, all_features,\n",
    "        response_method = \"predict\", ax = axis,\n",
    "        xlabel = all_features.columns[0], ylabel = all_features.columns[1],\n",
    "        grid_resolution = FIGURE_RESOLUTION,\n",
    "        cmap = 'RdBu', alpha = 0.50,\n",
    "    )\n",
    "\n",
    "    # Display the train data points.\n",
    "    axis.scatter(\n",
    "        train_features[train_features.columns[0]], train_features[train_features.columns[1]],\n",
    "        c = train_labels,\n",
    "        cmap = 'RdBu', alpha = 0.25, edgecolor = 'w',\n",
    "    )\n",
    "\n",
    "    # Display the test data points.\n",
    "    axis.scatter(\n",
    "        test_features[test_features.columns[0]], test_features[test_features.columns[1]],\n",
    "        c = test_labels,\n",
    "        cmap = 'RdBu', alpha = 0.75, edgecolor = 'k',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b6096-ef40-42e2-adab-82a6e89569af",
   "metadata": {},
   "source": [
    "We will also need to get some data before for us to classify.\n",
    "\n",
    "For this example, we will be generating some fake data using [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).\n",
    "This function is useful for quickly generating some classification test data.\n",
    "The data will be pretty simple, but works well as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077bf87-6cab-407b-a3e8-0aa498660366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 100 -- Make 200 data points.\n",
    "# n_features = 2 -- Generate two feature columns (perfect for plotting decision boundaries).\n",
    "# n_redundant = 0 -- No redundant features (features with the same information as other features).\n",
    "# n_informative = 2 -- Make our two features useful (and not just random).\n",
    "# random_state = 4 -- The seed for the random number generator.\n",
    "#                     The exact number doesn't matter, the same seed will generate the same data.\n",
    "# n_clusters_per_class = 1 -- Make the data simple.\n",
    "all_features, all_labels = sklearn.datasets.make_classification(\n",
    "    n_samples = 200, n_features = 2,\n",
    "    n_redundant = 0, n_informative = 2,\n",
    "    random_state = 4, n_clusters_per_class = 1\n",
    ")\n",
    "\n",
    "# Turn the features into a frame, the labels can stay as a list.\n",
    "all_features = pandas.DataFrame(all_features, columns = ['A', 'B'])\n",
    "\n",
    "# Split the data into train and test data.\n",
    "# Note that we are not being rigorous and making sure the splits have the same label breakdown.\n",
    "\n",
    "train_features = all_features[:100]\n",
    "train_labels = all_labels[:100]\n",
    "\n",
    "test_features = all_features[100:]\n",
    "test_labels = all_labels[100:]\n",
    "\n",
    "print(train_features[0:10])\n",
    "print('---')\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf983f69-18cf-46b1-9fdd-953fa47d1e0c",
   "metadata": {},
   "source": [
    "For our classifier in this example, we will be using a [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine) (SVM).\n",
    "SVMs are a classic and popular family of classifiers.\n",
    "We will not be getting into the details of SVMs here,\n",
    "but we will be using it because it tends to make clean decision boundaries that are easy for us to visualize.\n",
    "(It can also do a cool trick that we will see later in this example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29d312-f82e-4338-ad74-24d0e368b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a basic SVM classifier (no need to tweak any parameters).\n",
    "classifier = sklearn.svm.LinearSVC()\n",
    "\n",
    "# Fit the classifier on the training data.\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "visualize_decision_boundary(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc9659-fa7d-440b-8ec0-782fb913d606",
   "metadata": {},
   "source": [
    "## Decision Boundary vs Amount of Data\n",
    "\n",
    "When we have a fully trained classifier that gets 100% accuracy on the train data and 99% accuracy on the test data,\n",
    "we can see a very clear decision boundary with all the training points on their respctive sides.\n",
    "We can see a single test point on the wrong side, but even that point is really close to the decision boundary.\n",
    "\n",
    "But what about for a classifier that may not be trained all the way (or one that does not have nice clear data like ours)?\n",
    "How does the decision boundary change as we increase the amount of training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb5103-fe54-48ca-95e1-9bff726ad38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_range = [4, 8, 16, 32, 64, 100]\n",
    "\n",
    "for num_points in num_points_range:\n",
    "    classifier = sklearn.svm.LinearSVC()\n",
    "\n",
    "    train_sub_features = train_features[0:num_points]\n",
    "    train_sub_labels = train_labels[0:num_points]\n",
    "    \n",
    "    classifier.fit(train_sub_features, train_sub_labels)\n",
    "    visualize_decision_boundary(classifier,\n",
    "                                train_sub_features, train_sub_labels,\n",
    "                                test_features, test_labels,\n",
    "                                title = \"%d Points\" % (num_points))\n",
    "    \n",
    "    print(\"Accuracy: \", classifier.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff5f2c-532f-45aa-91e8-f08abcfc0c8b",
   "metadata": {},
   "source": [
    "We can see that generally, more training points leads to a decision boundary that is more general and works better on the test data.\n",
    "\n",
    "There is an interesting situation at 32 points where it creates a decision boundary that is worse than the one created at 16 points.\n",
    "With so few points, this behavior is not unexpected.\n",
    "We just got lucky at 16 points and happened to make a better decision boundary.\n",
    "As we add more points, the data makes up for the luck and eventually we get an even better decision boundary at 100 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a9a8e-36cc-4acd-b825-f2ce43abfe8b",
   "metadata": {},
   "source": [
    "## \"Linear\" Decision Boundaries\n",
    "\n",
    "Let's make some new data.\n",
    "This time, we will use the [sklearn.datasets.make_moons()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function.\n",
    "This function makes two-dimensional data that looks like two interleaving half circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285af341-df85-4365-9fc7-db63943fdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 100 -- Make 200 data points.\n",
    "# noise = 0.3 -- Control how random the data looks..\n",
    "# random_state = 4 -- The seed for the random number generator.\n",
    "#                     The exact number doesn't matter, the same seed will generate the same data.\n",
    "all_features, all_labels = sklearn.datasets.make_moons(n_samples = 200, noise = 0.3, random_state = 5)\n",
    "\n",
    "# Turn the features into a frame, the labels can stay as a list.\n",
    "all_features = pandas.DataFrame(all_features, columns = ['A', 'B'])\n",
    "\n",
    "# Split the data into train and test data.\n",
    "# Note that we are not being rigorous and making sure the splits have the same label breakdown.\n",
    "\n",
    "train_features = all_features[:100]\n",
    "train_labels = all_labels[:100]\n",
    "\n",
    "test_features = all_features[100:]\n",
    "test_labels = all_labels[100:]\n",
    "\n",
    "print(train_features[0:10])\n",
    "print('---')\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a38407-68c6-4482-b8fe-2392c86a7ff7",
   "metadata": {},
   "source": [
    "Now that we have new data, let's use the exact same classifier and process we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8acc1-c7e6-4b57-8d03-0201830374c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a basic SVM classifier (no need to tweak any parameters).\n",
    "classifier = sklearn.svm.LinearSVC()\n",
    "\n",
    "# Fit the classifier on the training data.\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "visualize_decision_boundary(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce4f7e-dbc2-4427-860c-78df0dc09509",
   "metadata": {},
   "source": [
    "With our previous data, we were able to get 99% accuracy,\n",
    "but with this data we only get 83% accuracy (which it pretty bad for a toy dataset).\n",
    "Here we can see one of the reason we have so many different classifiers and parameters for each classifier.\n",
    "How the data looks can dramatically affect the performance of a classifier and different classifiers can handle different types of data differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f0587-0336-48b6-a2ec-46a9fe8dd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a SVM classifier using an RBF kernel.\n",
    "classifier = sklearn.svm.SVC()\n",
    "\n",
    "# Fit the classifier on the training data.\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "visualize_decision_boundary(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a40df8-b820-4f4a-a47d-b25d21c888b2",
   "metadata": {},
   "source": [
    "Using a different member of the SVM family (one using a [Radial bias function](https://en.wikipedia.org/wiki/Radial_basis_function) (RBF) kernel),\n",
    "we can now get 91% accuracy without any other tweaks (which is pretty good).\n",
    "\n",
    "You may be asking why our decision boundary is no longer a line.\n",
    "Our first SVM classifier made a linear decision boundary,\n",
    "but this one (which is still an SVM) now makes some sort of blob-like decision boundary.\n",
    "\n",
    "The trick here is that this SVM changed it's definition of \"linear\" when classifying the data.\n",
    "To see this, think about Cartesian vs polar coordinate systems.\n",
    "\n",
    "In Cartesian coordinates, one of the most simple lines would be a horizontal line:\n",
    "$$\n",
    "    y = 2\n",
    "$$\n",
    "\n",
    "<center><img src=\"linear-cartesian.png\" width=400px /></center>\n",
    "<center style='font-size: small'>Image generated from <a href='https://www.desmos.com/calculator/mmoevgjbqg'>desmos</a>.</center>\n",
    "\n",
    "In polar coordinates, an equally simple line would be a circle:\n",
    "$$\n",
    "    r = 2\n",
    "$$\n",
    "\n",
    "<center><img src=\"linear-polar.png\" width=400px /></center>\n",
    "<center style='font-size: small'>Image generated from <a href='https://www.desmos.com/calculator/erzlxmnumw'>desmos</a>.</center>\n",
    "\n",
    "Both of these lines are considered \"linear\" in their respective spaces (coordinate systems),\n",
    "but look much more complex in other coordinate systems.\n",
    "Using these same tactics, our SVM transformed its feature space so that it can create decision boundaries that may look non-linear, but actually are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a86b2-5c22-42f7-8dbc-8ae24ae4597f",
   "metadata": {},
   "source": [
    "## More Classifiers\n",
    "\n",
    "For a look at decision boundaries from several different classifiers,\n",
    "see scikit-learn's [classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py).\n",
    "\n",
    "<center><img src=\"classifier-comparison.png\"/></center>\n",
    "<center style='font-size: small'>Image from <a href='https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py'>scikit-learn</a>.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dec82e-a370-42ca-b894-e7d9ad8a5a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

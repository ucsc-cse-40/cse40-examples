{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcecaea-b9fe-4346-9b1b-0d5d09aaf3f2",
   "metadata": {},
   "source": [
    "# Fairness\n",
    "\n",
    "Sometimes we create machine leaning models to see if an image is of a dog\n",
    "or models to score the cuteness of said dog.\n",
    "These predictions that these models produce are largely benign and have limited impact.\n",
    "But that will not always be true.\n",
    "As you progress as a machine learning professional,\n",
    "the time may come when you create models that can have serious impact on people's lives\n",
    "(and not always in expected ways).\n",
    "\n",
    "In this exercise, we start to explore the concept of fairness and bias in machine learning.\n",
    "We will see an example of real-world data that is biased,\n",
    "an example of synthetic data (inspired by real data) that is more subtly biased,\n",
    "and the complexity of a real-world machine learning controversy.\n",
    "\n",
    "This example will touch on the following topics:\n",
    " - Cleaning Real-World Data\n",
    " - Inspecting a Classifiers' Most Relevant Features\n",
    " - Columns that show a Direct Bias (Protected Attributes)\n",
    " - Columns that show an Indirect Bias\n",
    " - Columns that show a Distant Bias (Proxy Attributes)\n",
    " - Fairness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee781757-0dc0-4941-9b49-9b572aba96f8",
   "metadata": {},
   "source": [
    "## Census Data\n",
    "\n",
    "In this example, we will be using real-world data collected from the 1994 US Census database.\n",
    "It is called the [UCI Adult](https://archive.ics.uci.edu/ml/datasets/Adult) (or UCI Census) dataset,\n",
    "and is one of the most well-known and widely used datasets in the machine learning fairness community.\n",
    "It contains information about people and whether or not they make more than $50K a year (in 1994 dollars).\n",
    "The data has already been cleaned by the original authors, but still has some issues we will need to address.\n",
    "(So note that the data may look all clean in this nice neat notebook,\n",
    "but remember that there was someone (me) spending a few hours experimenting on (and cursing) the data to make sure it is clean (enough)).\n",
    "\n",
    "For the purposes of this example, a single column (`zipcode`) has been added to the dataset.\n",
    "This column has been synthetically generated (i.e. does not represent the actual zip code of the people represented in the data),\n",
    "but has been generated based on a real-life pattern that will be discussed later in this example.\n",
    "No other modification have been made to this dataset.\n",
    "\n",
    "Technically, the classification target of this data is predicting if the person earns more than $50K a year.\n",
    "However, you can easily see how this can be used as a proxy for other tasks.\n",
    "Historically, some related tasks of interest are approval for loans (business, home, and automobile) and approval for credit cards.\n",
    "All of these financial tasks can have real impact for a person and the result of a classifier can seriously change a person's life (like getting a loan for a business or house).\n",
    "It would not be a stretch for a future boss/manager to come to you (the resident machine learning expert) and saying something like:\n",
    "\"Here we have this public census data,\n",
    "see if you can find a way to predict if someone will earn enough in the future to be a safe bet for a loan.\"\n",
    "So as we go through this example, imagine that our actual prediction target is approval for a loan.\n",
    "\n",
    "Let's start as we often do in machine learning, with loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262437d-fac8-4796-8afb-c7d65bb14979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Note that our data is actually zipped up, but pandas\n",
    "# (as well as Python and most languages/libraries) can easily handle that.\n",
    "DATA_DIR = 'uci-census'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'adult.data.gz')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'adult.test.gz')\n",
    "\n",
    "# The column names are not in the actual data files,\n",
    "# but instead are listed in the description of the data.\n",
    "COLUMN_NAMES = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'zipcode',\n",
    "    # This column doesn't actually have a name, we just had to make one up.\n",
    "    'label',\n",
    "]\n",
    "\n",
    "# Load the training data.\n",
    "raw_train_data = pandas.read_csv(\n",
    "    TRAIN_PATH,\n",
    "    # Note that the column separator is not a comma (which it should be),\n",
    "    # instead it is a comma and a space (', ').\n",
    "    # Setting `sep` to more than one character also means we should set `engine`,\n",
    "    # or we will get a warning (try it out).\n",
    "    sep = ', ', engine = 'python',\n",
    "    # The column names are not in the data file.\n",
    "    header = None, names = COLUMN_NAMES,\n",
    ")\n",
    "\n",
    "# Load the test data.\n",
    "raw_test_data = pandas.read_csv(\n",
    "    TEST_PATH,\n",
    "    sep = ', ', engine = 'python',\n",
    "    header = None, names = COLUMN_NAMES,\n",
    "    # The first row in the test file is a header (but not column names).\n",
    "    skiprows = 1,\n",
    ")\n",
    "\n",
    "# For some stupid (and really frustrating) reason, the test data rows all end with a period.\n",
    "raw_test_data['label'] = raw_test_data['label'].str.rstrip('.')\n",
    "\n",
    "# Make sure that the zip code is treated as a string and not a number.\n",
    "raw_train_data['zipcode'] = raw_train_data['zipcode'].astype(str)\n",
    "raw_test_data['zipcode'] = raw_test_data['zipcode'].astype(str)\n",
    "\n",
    "raw_train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db4fbf-152c-4d41-bfad-2d40aeda9ba0",
   "metadata": {},
   "source": [
    "We loaded out data, and now we need to do some general cleaning and encoding.\n",
    "We will do this in a method so we can invoke it multiple times later in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7325624-4fa6-4d0c-a26a-c0c89df33529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for standard cleaning on the census data,\n",
    "# We will be needing to invoke this multiple times later.\n",
    "def clean_census_data(train_data, test_data):\n",
    "    # Temporarily combine the train and test data to ensure all cleaning and encoding is consistent.\n",
    "    frame = pandas.concat([train_data, test_data], ignore_index = True)\n",
    "    \n",
    "    # Make the label a boolean.\n",
    "    frame['label'] = frame['label'] == '>50K'\n",
    "\n",
    "    # Remove the label from the features.\n",
    "    labels = frame.pop('label')\n",
    "    \n",
    "    # Encode categorical columns.\n",
    "    # By default, pandas does not encode numeric columns.\n",
    "    frame = pandas.get_dummies(frame)\n",
    "    \n",
    "    # Scale numerical columns.\n",
    "    transformer = sklearn.preprocessing.StandardScaler()\n",
    "    \n",
    "    # We already one-hot encoded, but further normalizing the columns is a trick to help converge.\n",
    "    columns = list(frame.columns)\n",
    "    frame[columns] = transformer.fit_transform(frame[columns])\n",
    "    \n",
    "    # Split the data back into train/test.\n",
    "    # Note that we have been careful not to change the order of the data,\n",
    "    # so re-splitting is easy.\n",
    "    \n",
    "    train_features = frame[:len(train_data)]\n",
    "    train_labels = labels[:len(train_data)]\n",
    "    \n",
    "    test_features = frame[len(train_data):]\n",
    "    test_labels = labels[len(train_data):]\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = clean_census_data(raw_train_data,\n",
    "                                                                             raw_test_data)\n",
    "\n",
    "train_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68526ad4-7444-4614-b2f7-02de915c95dc",
   "metadata": {},
   "source": [
    "Now that we have some clean and split data, let's see how a basic classifier will perform without any tweaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631c75b-c6cd-4dcf-bc4f-9cae49fcc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "classifier.fit(train_features, train_labels)\n",
    "score = classifier.score(test_features, test_labels)\n",
    "\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21a85c-bab1-4096-a459-b0a7b96175f2",
   "metadata": {},
   "source": [
    "85% is a pretty good score to start with.\n",
    "We can surely do better if we tweak it around some,\n",
    "but let's start with something else.\n",
    "What are the important attributes/features for our classifier?\n",
    "\n",
    "We specifically choose a linear classifier (logistic regression in this case),\n",
    "so that it is very simple to see what the most important features are.\n",
    "We just have to look at the highest and lowest weights for each feature.\n",
    "\n",
    "Let's make a function for checking the important features\n",
    "(because we will probably be doing this a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b69d5-c653-437b-a79c-d561be65ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_important_features(classifier,\n",
    "                            train_features, train_labels, test_features, test_labels,\n",
    "                            count = 10):\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    score = classifier.score(test_features, test_labels)\n",
    "\n",
    "    print(\"Score: %5.4f\" % (score))\n",
    "\n",
    "    # Get the weights for each feature.\n",
    "    # Note that we are putting the weight and feature name in a tuple\n",
    "    # (with the weight first) so that we can sort them by weight easily.\n",
    "    feature_scores = []\n",
    "    for i in range(classifier.n_features_in_):\n",
    "        feature_scores.append((classifier.coef_[0][i], classifier.feature_names_in_[i]))\n",
    "    \n",
    "    feature_scores = list(sorted(feature_scores, reverse = True))\n",
    "    print(\"Top Positive Features\")\n",
    "    for i in range(count):\n",
    "        print(\"    % 4.2f -- %s\" % feature_scores[i])\n",
    "    \n",
    "    feature_scores = list(sorted(feature_scores, reverse = False))\n",
    "    print(\"Top Negative Features\")\n",
    "    for i in range(count):\n",
    "        print(\"    % 4.2f -- %s\" % feature_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cddd8-a1f5-4431-b266-c44582bebe8b",
   "metadata": {},
   "source": [
    "Now let's see what our important features actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751ec0c-35aa-41d9-a44a-b776d773a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "show_important_features(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f512dc-aae3-45e0-bbc1-c92c6fb82abb",
   "metadata": {},
   "source": [
    "Okay, we can see some stuff that makes sense (and some stuff that seems really troublesome).\n",
    "\n",
    "For the top three positive features, we can see some stuff that really makes sense:\n",
    " - capital gains -- I feel like someone who is making capital gains of stocks will probably be making good money.\n",
    " - Married -- Sure. I think I remember hearing somewhere that married people tend to make more (especially if the income is actually household income).\n",
    " - Education -- Okay, people with higher levels of education tend to make more money.\n",
    "\n",
    "Some of the top negative features makes sense:\n",
    " - Not Married -- Maybe the opposite of the married we saw in the positive features.\n",
    " - Preschool Education -- People who have not had an opportunity to go to school probably have less opportunities to make money.\n",
    " - Zip Code -- Maybe that is a poorer area?\n",
    "\n",
    "But then there are also ones that seem REALLY bad to have in our model,\n",
    "first among them being \"sex_Female\".\n",
    "It would be not just morally wrong to build a model that says predicts lower income (and rejects loan applications) because an applicant in female,\n",
    "but it is also illegal in the US (and you can bet this would be super illegal under EU regulations).\n",
    "The [Equal Credit Opportunity Act (ECOA)](https://www.ftc.gov/legal-library/browse/statutes/equal-credit-opportunity-act)\n",
    "> ... prohibits discrimination on the basis of race, color, religion, national origin, sex, marital status, age, receipt of public assistance, or good faith exercise of any rights under the Consumer Credit Protection Act.\n",
    "\n",
    "Also note that the Consumer Financial Protection Bureau (CFPB) has\n",
    "[clarified that sexual orientation and gender are also covered by the ECOA](https://www.consumerfinance.gov/about-us/newsroom/cfpb-clarifies-discrimination-by-lenders-on-basis-of-sexual-orientation-and-gender-identity-is-illegal/).\n",
    "Also keep in mind that this data is from 1994, so it uses outdated concepts like sex as a proxy for gender.\n",
    "\n",
    "When we have features in a model that we cannot use morally or legally,\n",
    "we call these features \"protected attributes\".\n",
    "Let's go ahead and remove all those protected attributes from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251b269-2294-4f52-9cd4-ebe2d1b5012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_train_data.copy()\n",
    "test_data = raw_test_data.copy()\n",
    "\n",
    "REMOVE_COLUMNS = [\n",
    "    'age',\n",
    "    'marital-status',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "]\n",
    "\n",
    "train_data = train_data.drop(columns = REMOVE_COLUMNS)\n",
    "test_data = test_data.drop(columns = REMOVE_COLUMNS)\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = clean_census_data(train_data, test_data)\n",
    "\n",
    "train_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a783785-fe6e-4dd6-9b0c-8119a30bef8d",
   "metadata": {},
   "source": [
    "Now how does our model perform (and what are our new important features) now that we have removed the protected attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1602f-2370-4af1-8d95-99051566d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "show_important_features(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63c0cd-00f4-4b6e-8d5a-03412e803693",
   "metadata": {},
   "source": [
    "Looks like our model performs about the same without the biased columns.\n",
    "Note that the accuracy is slightly lower, but we are also not doing any cross-validation so we will treat the results a little looser.\n",
    "\n",
    "In some machine learning scenarios (usually not the ones you will see in this class),\n",
    "we will even sometimes see better results for models that are more fair.\n",
    "This is because enforcing fairness in a model can make a model more robust,\n",
    "and robust models tend to generalize better and perform better on unseen data.\n",
    "\n",
    "Of course, we no longer see the protected attributes that we removed.\n",
    "However, we do see columns like \"relationship_Husband\" and \"relationship_Wife\".\n",
    "Of course, this violates the marital status provision in the ECOA, but these columns also indirectly violate the sex provision.\n",
    "Let's look more closely at the values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19ca1e-3e7a-4421-b274-9105f4ea0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['relationship'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9a349-85e4-4ef8-9406-3272984b4347",
   "metadata": {},
   "source": [
    "Seems like this is a column that should be removed.\n",
    "But **just for teaching purposes**, let's do something else instead.\n",
    "Let's pretend that marital status is not a protected attribute.\n",
    "If marital status is not protected, then the problem with the \"relationship\" column is that it exposes information about a protected attribute (sex).\n",
    "In this way, we are leaking protected information into our model.\n",
    "\n",
    "Usually, we just get rid of anything that has or leaks protected information.\n",
    "But Sometimes, if we are lucky, we may be able to remove the protected information without removing all the information in the column.\n",
    "We call this \"debiasing\" our data.\n",
    "\n",
    "What if we merge the 'Husband' and 'Wife' values into a single 'Spouse' value?\n",
    "Then we can remove the sex information without loosing the other information from this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fb6d1-d1aa-4a5e-869b-854f7e4a2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Husband' and 'Wife' in the \"relationship' column with 'Spouse'.\n",
    "train_data.loc[train_data['relationship'].isin(['Husband', 'Wife']), 'relationship'] = 'Spouse'\n",
    "test_data.loc[test_data['relationship'].isin(['Husband', 'Wife']), 'relationship'] = 'Spouse'\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = clean_census_data(train_data, test_data)\n",
    "\n",
    "# Look at the relationship column.\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eea9a3-6e96-4305-8dba-432695c51881",
   "metadata": {},
   "source": [
    "Now how does our classifier perform with the new debiased \"relationship\" column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867a4bd-9d71-40b2-8f74-2f56fe483da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "show_important_features(classifier, train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b0790-b8b9-4bbf-8208-7d13aba1f4ed",
   "metadata": {},
   "source": [
    "Great, we merged the offending values and still do about the same.\n",
    "We even still see the debiased column in our top positive features.\n",
    "This means that the column is useful even when sex information is removed\n",
    "(e.g., the biased/protected information is not necessary for the model, and the debiased information is useful).\n",
    "\n",
    "So, are we now done?\n",
    "Did we remove all the bias in our dataset and make it super fair?\n",
    "You can make some valid arguments about the fairness of using some of the information we still have like nation of origin,\n",
    "but there is something else that really stands out.\n",
    "\n",
    "If you look back to our most recent important features,\n",
    "there is one zip code entry that is the top negative feature.\n",
    "Note that this column is not all zip codes (since we one-hot encoded them),\n",
    "this is just a single zip code (30940).\n",
    "It's the only zip code we see in either the positive or negative features.\n",
    "But this could make sense.\n",
    "We know that there are affluent and impoverished areas.\n",
    "It makes sense that a zip code could help predict how much someone earns\n",
    "(e.g., you have to make more money to live in a rich area).\n",
    "\n",
    "But is that all there is to this specific zip code?\n",
    "Unfortunately, no.\n",
    "There is something much more depressing going on here.\n",
    "Let's look closer at the people in this zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef92b0-2436-44b3-a1d7-781a184b5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[raw_train_data['zipcode'] == '30940']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236f1b6-8664-4b08-9b11-581d93cd0c3f",
   "metadata": {},
   "source": [
    "We can see that they mostly have the negative label,\n",
    "but we already know that the zip code is a good predictor of this label.\n",
    "Aside from that, it's hard to see any other specific patterns.\n",
    "\n",
    "But, here we are looking at the clean data,\n",
    "what if we go all the way back to the raw data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa4643-74d1-4ee3-95aa-e988c53fcbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data[raw_train_data['zipcode'] == '30940']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5550a4-93d8-44d3-af0a-94f31bd4205a",
   "metadata": {},
   "source": [
    "It looks like all (or almost all) the people in this zip code are black.\n",
    "Let's look closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daf2df-7070-44f3-9ac1-7d4d71b7fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data[raw_train_data['zipcode'] == '30940']['race'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f930b8c-24c9-4294-9ec3-f72cbd65dd6e",
   "metadata": {},
   "source": [
    "Clearly, this zip code is predominantly black,\n",
    "but this could just match the overall distribution by race for the entire dataset.\n",
    "What if the census data was taken in a predominately black city.\n",
    "\n",
    "Let's look at the breakdown by race for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fffa7-4cb0-460e-9f6a-046e11ab9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data['race'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8a46b-444c-44d1-8bda-87b1303144e4",
   "metadata": {},
   "source": [
    "Wow.\n",
    "This dataset is heavily weighted towards white participants.\n",
    "But even with that weighting, the zip code we were looking at is still predominantly black.\n",
    "And not only is the zip code predominantly black,\n",
    "but it is also a strong indicator for lower income.\n",
    "\n",
    "What about the other zip codes?\n",
    "Do they also follow a similar pattern?\n",
    "Let's plot a histogram by race for all our zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933de31c-6cc8-4c7d-9524-b49e04da5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = matplotlib.pyplot.figure(figsize = [15, 15])\n",
    "zipcodes = sorted(raw_train_data['zipcode'].unique())\n",
    "\n",
    "# Loop through each zip code.\n",
    "for i in range(len(zipcodes)):\n",
    "    axis = figure.add_subplot(math.ceil(len(zipcodes) / 3), 3, i + 1)\n",
    "    \n",
    "    raw_train_data[raw_train_data['zipcode'] == zipcodes[i]]['race'].sort_values().hist(ax = axis)\n",
    "    axis.set_title(\"zipcode = %s\" % (zipcodes[i]))\n",
    "                   \n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df71a3a-a20b-4b64-8e17-c8e04eb70c55",
   "metadata": {},
   "source": [
    "It looks like it is only that one zip code (30940) specifically that is biased.\n",
    "All the others look about the same as the overall dataset.\n",
    "\n",
    "What we discovered here is an attribute which has certain values that can be used as proxy for a protected attribute.\n",
    "Therefore, we have discovered that our model is still unfair because it is getting a signal from a protected attribute.\n",
    "\n",
    "But why are we seeing this?\n",
    "How is zip code predicting race?\n",
    "\n",
    "What we see here is actually something we see all over the United States.\n",
    "In certain places, specific zip codes can be a strong predictor for race because of a practice called [redlining](https://en.wikipedia.org/wiki/Redlining).\n",
    "Redlining is a discriminatory behavior that has (and still is) used to keep people considered \"undesirable\" (a thinly coded word usually targeted towards black homeowners) out of areas with predominately white residents.\n",
    "This forced black potential homeowners into areas that were already predominantly black.\n",
    "These areas were then starved of investment which affords less opportunity to the residents and less access to financial and social mobility.\n",
    "Consider the cycle of redlining:\n",
    " 1) You want to buy a house.\n",
    " 2) You come from a redlined zip code, so the bank will not give you good rates for a home loan in a non-redlined area and relators may not cooperate with you in non-redlined areas.\n",
    " 3) You are forced to buy a house in a redlined zip code that is considered \"low income\".\n",
    " 4) Because the area is \"low income\", people with money do not move there.\n",
    " 5) Because of lack of investment, low property values (and therefore property taxes), and lack of outsides moving in, the area has poor infrastructure like schools, grocery stores, and public transit.\n",
    " 6) You want to start a business, but because you lived in a redlined zip code the bank denies your loan (maybe it was using the biased model we just built!).\n",
    " 7) Your kids don't have the educational resources or community investment growing up and therefore have low financial and social mobility.\n",
    " 8) Your kids now start back at #1.\n",
    "\n",
    "Redlining is a form of quasi-legal segregation that was heavily practiced during reconstruction and through the civil rights era,\n",
    "and is still more subtly practiced today.\n",
    "\n",
    "Because of systematic racism, our \"zipcode\" column is biased in a very subtle way.\n",
    "And because we are using a biased column, our model is not biased and unfair.\n",
    "Machine learning can be hard and it can affects real people.\n",
    "We always need to do our best to find places that are unfair and work to make them more fair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17b476-4a34-404c-9410-1c4d98d42bd8",
   "metadata": {},
   "source": [
    "## COMPAS\n",
    "\n",
    "In the US justice system, before you are sentenced, paroled, or the terms of your bail are set\n",
    "(all steps that may result in incarceration)\n",
    "a judge or committee is supposed to assess your risk for future crime (nonviolent or violent).\n",
    "If you are considered low risk you may get favorable (a lower sentence, early parole, a low bail),\n",
    "but if you are considered high rick then you get the opposite treatment.\n",
    "COMPAS is software that was developed to do this risk assessment instead of humans.\n",
    "\n",
    "As a promising machine learner,\n",
    "you should already be very cautious about a system that decides whether people will spend more time in jail.\n",
    "(Especially since we are discussing it during a fairness example.)\n",
    "\n",
    "The nonprofit organization [ProPublica](https://en.wikipedia.org/wiki/ProPublica)\n",
    "did a [study on COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) and found it to be racist.\n",
    "ProPublica found that COMPAS was more likely to incorrectly label black defendants as high risk,\n",
    "and less likely to label white defendants as high risk.\n",
    "In more precise terms, if COMPAS' predictive task was a binary classification where high right is the positive label,\n",
    "black defendants have a higher false positive rate (FPR)\n",
    "while white defendants have a higher false negative rate (FNR).\n",
    "NorthPoint, the company that makes COMPAS, responded that the predictive parity between the groups is the same.\n",
    "\n",
    "What we see here is not so much an argument about the data (or even the model),\n",
    "but really an argument about what metrics are most appropriate for this situation.\n",
    "We have already talked about evaluation metrics and how there are [dozens of them](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "But here we are talking about [fairness metrics](https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Mathematical_formulation_of_group_fairness_definitions)\n",
    "(measures of how fair a machine learning model is).\n",
    "Like evaluation metrics, there are many different fairness metrics.\n",
    "In fact, if we consider fairness to be defined by a metric, then there are at least 14 different definitions of fairness!\n",
    "To make it even more difficult, it is mathematically impossible to guarantee that all fairness metrics can be satisfied at the same time.\n",
    "\n",
    "As with many things in machine learning, this situation comes down to trade-offs and expertise.\n",
    "There is not an automatic way to just say: this is the right fairness metric.\n",
    "It is up to you as the expert to decide what is the most appropriate set of fairness metrics to use in a specific scenario.\n",
    "The \"right\" answer is to usually pick a set of fairness metrics that best fit your scenario\n",
    "while always keeping in mind how an unfair model can change people's lives.\n",
    "\n",
    "Here are some resources if you want to read more about the COMPAS controversy:\n",
    " - [Wikipedia Entry for the COMPAS Software](https://en.wikipedia.org/wiki/COMPAS_(software))\n",
    " - [Lecture Slides that Discuss COMPAS and Fairness](https://web.stanford.edu/class/cs329t/slides/fairness-Week1.pdf)\n",
    " - [Initial ProPublica Article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n",
    "   - [More Information on ProPublica Methodology](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db240e30-b59b-4e46-b7b8-0ba31afba4f2",
   "metadata": {},
   "source": [
    "Now that we have talked about fairness metrics,\n",
    "let's see what fairness metrics will say about our census data.\n",
    "\n",
    "Let's start by making a function that outputs a confusion matrix and stats for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d481f-a8dd-40e4-96ad-2bd5be681ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(train_features, train_labels, test_features, test_labels, title = None):\n",
    "    classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    test_predictions = classifier.predict(test_features)\n",
    "    sklearn.metrics.ConfusionMatrixDisplay.from_estimator(classifier, test_features, test_labels,\n",
    "                                                          labels = [False, True],\n",
    "                                                          display_labels = ['Low Income',\n",
    "                                                                            'High Income'],\n",
    "                                                          cmap = 'Blues', colorbar = False)\n",
    "    matplotlib.pyplot.title(title)\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    # Output all sorts of stats.\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(test_labels, test_predictions).ravel()\n",
    "    p = tp + fn\n",
    "    n = tn + fp\n",
    "    count = p + n\n",
    "    \n",
    "    print(\"Accuracy         : %5.4f\" % ((tp + tn) / count))\n",
    "    print()\n",
    "    print(\"Recall (TPR)     : %5.4f\" % (tp / p))\n",
    "    print(\"Miss Rate (FNR)  : %5.4f\" % (fn / p))\n",
    "    print()\n",
    "    print(\"Fall-Out (FPR)   : %5.4f\" % (fp / n))\n",
    "    print(\"Selectivity (TNR): %5.4f\" % (tn / n))\n",
    "    print()\n",
    "    print(\"Precision        : %5.4f\" % (tp / (tp + fp)))\n",
    "    print()\n",
    "    print(\"Positive Rate    : %5.4f\" % ((tp + fp) / count))\n",
    "    print(\"Negative Rate    : %5.4f\" % ((tn + fn) / count))\n",
    "\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ca604-685a-4495-bf26-dc0cbe94f16f",
   "metadata": {},
   "source": [
    "Let's run these stats over our full test data to get a baseline for what to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3bf1a-324c-4bd3-906c-c6317dc9313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(train_features, train_labels, test_features, test_labels, title = 'All Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a479cb-8eb8-47c0-bf13-2832a22d7be2",
   "metadata": {},
   "source": [
    "When discussing redlining and COMPAS, we were talking about race.\n",
    "But I know (from exploring the data) that this dataset has another huge bias.\n",
    "Let's look at the metics broken up by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d0684-63ff-4201-b818-1b5260ae5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out test records by sex.\n",
    "male_test_features = test_features[(raw_test_data['sex'] == 'Male').values]\n",
    "male_test_labels = test_labels[(raw_test_data['sex'] == 'Male').values]\n",
    "\n",
    "female_test_features = test_features[(raw_test_data['sex'] == 'Female').values]\n",
    "female_test_labels = test_labels[(raw_test_data['sex'] == 'Female').values]\n",
    "\n",
    "show_confusion_matrix(train_features, train_labels, male_test_features, male_test_labels,\n",
    "                      title = 'Male')\n",
    "show_confusion_matrix(train_features, train_labels, female_test_features, female_test_labels,\n",
    "                      title = 'Female')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0aada-c7a2-43f0-b1a0-eb45efa74872",
   "metadata": {},
   "source": [
    "Pay particular attention to the values for False Positive Rate (FPR) and False Negative Rate (FNR)\n",
    "(these metrics were called out in our COMPAS discussion).\n",
    "In this case the FPR means that the system erred in the participants favor and predicted they have a higher income (and approved them for a loan),\n",
    "while the FNR means that the system erred against the participant and incorrectly predicted they had a low income (and denied their loan).\n",
    "\n",
    "The FPR for male participants is 0.0959 while the FPR for female participants is 0.0149.\n",
    "While the FNR for male participants is 0.3953 while the FNR for female participants is 0.5424.\n",
    "\n",
    "|        | FPR    | FNR    |\n",
    "|--------|--------|--------|\n",
    "| Male   | 0.0959 | 0.3953 |\n",
    "| Female | 0.0149 | 0.5424 |\n",
    "\n",
    "This means the model is about six times more likely to make a mistake in a man's favor (FPR) than in a women's,\n",
    "and 30% more likely to incorrectly decide against a woman than a man.\n",
    "\n",
    "Here we can see that is is important to not just consider how well our model can do,\n",
    "but also what happens when our classifier messes up.\n",
    "Our classifier may have a higher accuracy for female participants,\n",
    "but when it messes up on women it disadvantages them at higher rates than men.\n",
    "\n",
    "A more formal way we can see this disparity is by using a fairness metric.\n",
    "In this case, we will a fairly popular one: [demographic parity](https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Definitions_based_on_predicted_outcome) (also called statistical parity).\n",
    "Demographic parity look at if the protected and unprotected groups have equal probability of getting a positive label.\n",
    "\n",
    "For our task, we can use demographic parity to measure disparity using:\n",
    "$$\n",
    "    \\frac{ P( Income > 50K | Sex = Female ) }{ P( Income > 50K | Sex = Male ) }\n",
    "$$\n",
    "\n",
    "Thankfully, we already computed these values above (the positive rate for the respective sexes).\n",
    "$$\n",
    "    \\frac{ P( Income > 50K | Sex = Female ) }{ P( Income > 50K | Sex = Male ) } =\n",
    "    \\frac{ 0.0631 }{ 0.2484 } =\n",
    "    0.2540 \n",
    "$$\n",
    "\n",
    "1.0 would be considered perfectly fair (and a 20% margin is often given to be \"fair enough\").\n",
    "But we are far from that at 0.25.\n",
    "\n",
    "Let's compare this result against two values that should be fair,\n",
    "two of the unbiased zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bd905-8dba-4540-b03b-f96c19cb8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out test records by zip.\n",
    "zip1_test_features = test_features[(raw_test_data['zipcode'] == '11810').values]\n",
    "zip1_test_labels = test_labels[(raw_test_data['zipcode'] == '11810').values]\n",
    "\n",
    "zip2_test_features = test_features[(raw_test_data['zipcode'] == '13523').values]\n",
    "zip2_test_labels = test_labels[(raw_test_data['zipcode'] == '13523').values]\n",
    "\n",
    "show_confusion_matrix(train_features, train_labels, zip1_test_features, zip1_test_labels,\n",
    "                      title = '11810')\n",
    "show_confusion_matrix(train_features, train_labels, zip2_test_features, zip2_test_labels,\n",
    "                      title = '13523')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f0ed1-cfca-4b9e-a1b0-6b364201157e",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\frac{ P( Income > 50K | zipcode = 11810 ) }{ P( Income > 50K | zipcode = 13523 ) } =\n",
    "    \\frac{ 0.1858 }{ 0.1843 } =\n",
    "    1.0081\n",
    "$$\n",
    "\n",
    "As expected, for two fair values (two arbitrary zip codes),\n",
    "we can see almost no disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2278e23-af09-43b0-9275-1a9477af46ef",
   "metadata": {},
   "source": [
    "As with pretty much everything we have seen in machine learning,\n",
    "fairness is complicated.\n",
    "There are no clear answers, and it requires careful attention from the machine learning expert (you) to ensure that you models are free of biased data and fair.\n",
    "Choosing the right data and fairness metrics can change how a model is viewed and how it predicts.\n",
    "\n",
    "To conclude, I will leave you with a quote [of disputed origins](https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics).\n",
    "Consider this quote in the context of the data and fairness metrics you can choose for your models:\n",
    "> There are three kinds of lies: lies, damned lies, and statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

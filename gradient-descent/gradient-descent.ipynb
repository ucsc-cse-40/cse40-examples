{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85549633-ee86-4493-81f4-f37cd9ae2e87",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a simple algorithm at the heart of many (if not most) machine learning algorithms and concepts.\n",
    "Just like in calculus, the goal is to use a function's derivative (gradient) to find an extreme point (usually a global minimum) for that function.\n",
    "\n",
    "In this example, we will cover:\n",
    " - The basic concept of gradient descent.\n",
    " - An aside on plotting functions.\n",
    " - What gradient descent looks like on a simple function.\n",
    " - How different learning rates change gradient descent's behavior.\n",
    "\n",
    "For this exercise, we will be working with the simple function:\n",
    "$$\n",
    "    f(x) = x^2 - 4x\n",
    "$$\n",
    "\n",
    "Keep in mind that most applications in machine learning will be dealing with many more dimensions,\n",
    "but the same concepts will apply.\n",
    "\n",
    "Let's begin by getting some data that follows our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ccc4b-c4f8-47f8-9277-87344d357b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# Get 1000 evenly spaced points between -2 and 6.\n",
    "# (we chose those numbers so they will look good on a plot for this specific function).\n",
    "X = numpy.linspace(-2, 6, 1000)\n",
    "\n",
    "# f(x) = x^2 - 4x\n",
    "# Note that this function works for both scalars and numpy.ndarrays.\n",
    "def f(x):\n",
    "    return (x ** 2) - (4 * x)\n",
    "\n",
    "# Construct the y values from the x values.\n",
    "Y = f(X)\n",
    "\n",
    "print(X[0:20])\n",
    "print(Y[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b985d3e-7a16-4d71-b129-8240a971fe96",
   "metadata": {},
   "source": [
    "Let's plot our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a8ab-49dc-4935-a56a-78adccc768c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(X, Y)\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc632b-fcd2-4901-91d4-55e6bc82bd0d",
   "metadata": {},
   "source": [
    "## An Aside on Plotting Functions\n",
    "\n",
    "In general, computers are not good at handling abstract mathematical equations/functions.\n",
    "Mathematical equations are for people, computers want data points.\n",
    "So when we graph \"functions\", what we are typically doing is evaluating that function at many input values and creating a list of data points that can then be plotted.\n",
    "With enough data points, the resulting graph can look smooth/accurate.\n",
    "Even your old pal, the TI-83/89 calculators, did not graph lines directly.\n",
    "\n",
    "See below how the number of points affects how smooth the plot looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2828f1-c762-4a8e-80a2-5b5fbae5410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points = [2, 4, 8, 16, 32, 64]\n",
    "\n",
    "for count in number_of_points:\n",
    "    x_test = numpy.linspace(-2, 6, count)\n",
    "    y_test = f(x_test)\n",
    "    \n",
    "    matplotlib.pyplot.plot(x_test, y_test)\n",
    "    matplotlib.pyplot.title(\"%d Points\" % (count))\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd07523-f444-45cb-9d69-3f44181c2ab4",
   "metadata": {},
   "source": [
    "## Back to Gradient Descent\n",
    "\n",
    "Now that we have our function plotted, let's see how gradient descent can get us to the minimum.\n",
    "\n",
    "Let's choose a starting point $ x_0 = -1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a2bd5-eeac-4733-82d6-9172fbb13f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = -1\n",
    "\n",
    "matplotlib.pyplot.plot(X, Y)\n",
    "matplotlib.pyplot.plot(x_0, f(x_0), 'rx')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02823fe7-49d7-4b56-ac03-52c635b63e20",
   "metadata": {},
   "source": [
    "Now we will need to compute the gradient of our function:\n",
    "$$ \\begin{align}\n",
    "    f(x)        &= x^2 - 4x \\\\\n",
    "    \\nabla f(x) &= 2x - 4 \\\\\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db548bb-feb5-49a0-9b95-2f1f5fde9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'(x) = 2x - 4\n",
    "def gradient_f(x):\n",
    "    return (2 * x) - 4\n",
    "\n",
    "print(\"Starting point (x_0): (%f, %f)\" % (x_0, f(x_0)))\n",
    "print(\"Gradient at x_0: %f\" % (gradient_f(x_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c697c-04db-4b7e-9206-656741b6aed5",
   "metadata": {},
   "source": [
    "Recall that gradient descent computes the next point ($ x_{n + 1} $) using the equation:\n",
    "$$\n",
    "    x_{n + 1} = x_n - \\alpha \\nabla f(x_n)\n",
    "$$\n",
    "Where $ \\alpha $ is the *learning rate*, which decides how far we will move in one step.\n",
    "\n",
    "Let's arbitrarily pick a learning rate of 0.1.\n",
    "(Note that this learning rate is not actually arbitrary, I know that it will work well. We will cover setting a learning rate later.)\n",
    "\n",
    "With our starting point, learning rate, and gradient all figured out,\n",
    "we can compute the next point to visit.\n",
    "\n",
    "$$ \\begin{align}\n",
    "    x_{n + 1} &= x_n - \\alpha \\nabla f(x_n) \\\\\n",
    "    x_{1}     &= -1 - (0.1 * \\nabla f(x_0) \\\\\n",
    "    x_{1}     &= -1 - (0.1 * (2 x_0 - 4) \\\\\n",
    "    x_{1}     &= -1 - (0.1 * (2 * -1) - 4) \\\\\n",
    "    x_{1}     &= -1 - (0.1 * -6) \\\\\n",
    "    x_{1}     &= -1 - -0.6 \\\\\n",
    "    x_{1}     &= -0.4 \\\\\n",
    "\\end{align} $$\n",
    "\n",
    "Therefore, we know that the next $ x $ we will visit is $ -0.4 $,\n",
    "where our function has a value of $ f(-0.4) = 1.76 $\n",
    "(which is lower than the $ f(-1) = 3.0 $ that we were at in the initial location).\n",
    "\n",
    "We can also do all the above in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9384b-f13f-487b-94af-ff1bda94bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x_1 = x_0 - (alpha * gradient_f(x_0))\n",
    "\n",
    "print(\"Next point (x_1): (%f, %f)\" % (x_1, f(x_1)))\n",
    "\n",
    "matplotlib.pyplot.plot(X, Y)\n",
    "matplotlib.pyplot.plot(x_0, f(x_0), 'rx')\n",
    "matplotlib.pyplot.plot(x_1, f(x_1), 'rx')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c64369-6451-49b7-8ce6-f33202a4eeff",
   "metadata": {},
   "source": [
    "We have moved closer to the minimum!\n",
    "\n",
    "Now that we have done this process one by hand, let's make a function to do it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c7ba9-2c5d-49a3-92e9-230388e1812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(function, gradient, starting_point,\n",
    "                     learning_rate = 0.1, num_steps = 20, quiet = True):\n",
    "    # Keep track of all the points we visited.\n",
    "    history = [starting_point]\n",
    "    \n",
    "    x = starting_point\n",
    "    \n",
    "    if (not quiet):\n",
    "        print(\"Starting at (%f, %f)\" % (x, function(x)))\n",
    "          \n",
    "    for step in range(num_steps):\n",
    "        x_next = x - (learning_rate * gradient(x))\n",
    "        \n",
    "        if (not quiet):\n",
    "            print(\"Moved to (%f, %f)\" % (x_next, function(x_next)))\n",
    "        \n",
    "        history.append(x_next)\n",
    "        x = x_next\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c75d5b-d598-4906-b06b-82931a8a8215",
   "metadata": {},
   "source": [
    "This function will do `num_steps` iterations of gradient descent for us and return a list of all the points visited.\n",
    "(Note that this implementation relies on function objects that are passed in for the base and gradient functions, which means it is not specific to $ f(x) = x^2 - 4x $.)\n",
    "\n",
    "Let's try our our new gradient descent implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426002c2-0350-4f0b-9979-f82312e589bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gradient_descent(f, gradient_f, x_0, learning_rate = alpha, quiet = False)\n",
    "\n",
    "matplotlib.pyplot.plot(X, Y)\n",
    "matplotlib.pyplot.plot(history, f(numpy.array(history)), 'rx')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea676bc-a8b4-4775-af42-4a58550a359e",
   "metadata": {},
   "source": [
    "Fantastic!\n",
    "It looks like we were able to make it all the way to the minimum of $ f(x) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c3a7f-a2a3-4ee5-bd7b-824153dd926e",
   "metadata": {},
   "source": [
    "## Learning Rates\n",
    "\n",
    "Choosing a good learning rate for gradient descent can be pretty important to how well it will function.\n",
    "If you pick a learning rate that is too low, gradient descent can take a long time to find the right answer.\n",
    "But if you pick a learning rate that is too large, gradient descent can behave strangely and maybe never find a good answer.\n",
    "\n",
    "To see this in action, let's try our a few different learning rates.\n",
    "But first, we will make a function to speed up our testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201a474-429e-4728-af0d-1b9e24bfcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_learning_rate(learning_rate, num_steps = 20, quiet = True):\n",
    "    history = gradient_descent(f, gradient_f, x_0,\n",
    "                               learning_rate = learning_rate, num_steps = num_steps,\n",
    "                               quiet = quiet)\n",
    "\n",
    "    matplotlib.pyplot.plot(X, Y)\n",
    "    matplotlib.pyplot.plot(history, f(numpy.array(history)), 'rx')\n",
    "    matplotlib.pyplot.title(\"Learning Rate: %f, Steps: %d\" % (learning_rate, num_steps))\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795bac3-93c6-4604-86ec-8cb0402777cc",
   "metadata": {},
   "source": [
    "Now we will start with a learning rate that we have already seen will work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed863191-e038-4ef6-a844-2d8e1a9b6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eb63e-a58d-4fee-84a3-c81d249e0970",
   "metadata": {},
   "source": [
    "Looks fine.\n",
    "What happens when we decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fee7f7-92bd-4590-9ecf-7de53cac927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70065346-9746-4e03-89cf-85dfdf879026",
   "metadata": {},
   "source": [
    "We don't make it all the way down to the minimum.\n",
    "We will have to do more steps if we want to get all the way to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb962ab-7c81-482f-a637-63a9efd25ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.01, num_steps = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8a9f9-ca1b-467c-8ed4-5eb51638803b",
   "metadata": {},
   "source": [
    "Because of the decreasing nature of the gradient, even through it looks like 20 steps got us half the way we need far more than double the number of steps to get all the way to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ce495-3745-4180-9b22-8fd2009dbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.01, num_steps = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac7716-0b4c-444a-8eff-e7f9c9228275",
   "metadata": {},
   "source": [
    "Now let's see what happens with a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0484503-dd2c-4b9e-b6a6-053c55c155f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(1.0, quiet = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4949f57-778c-48b3-898c-445b86465188",
   "metadata": {},
   "source": [
    "That's pretty bad ...\n",
    "\n",
    "With this learning rate, gradient descent just keeps bouncing around between (-1, 5) and (5, 5).\n",
    "With this learning rate, we will never converge to an answer.\n",
    "\n",
    "If we set the learning rate just slightly higher, it gets even worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47e1bd-2147-44dd-8601-d367dd4b81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(1.01, quiet = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3f8dc-9ca4-42c7-8d18-3fdcde30f3ab",
   "metadata": {},
   "source": [
    "We will just keep moving further away from our goal.\n",
    "\n",
    "With a slightly smaller step size we will get closer to the minimum, but very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f1f63-6283-4736-884a-7206fe76d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.99, num_steps = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd91db-4cab-41ed-9503-b88caa11d200",
   "metadata": {},
   "source": [
    "When we choose a step size that is too large but still reasonable,\n",
    "then we may overshoot the minimum, but eventually converge to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9eddfc-71be-43ea-8bb3-4057e54e4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_learning_rate(0.75, quiet = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c02ef14-7fbb-4e46-b74b-62c0f04175cc",
   "metadata": {},
   "source": [
    "# Data Splits\n",
    "\n",
    "In data science and machine learning, data is our lifeblood.\n",
    "It is at the center of pretty much everything we do.\n",
    "So, this means that we usually want to use as much data as we can get.\n",
    "However, we can actually make better (more robust) models if we partition our data and don't use all of it at once.\n",
    "We call this \"splitting\" (or creating \"splits\" in) our data.\n",
    "In this exercise, we will cover the basics of splitting a tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7ce9d-a49d-4b47-87ce-3ba151b709f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we will need for this example.\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn.base\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123b8d0-e258-4e35-8136-ca90987f2773",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Consider the following two-dimensional data where each point is labeled as either red or blue.\n",
    "\n",
    "<center><img src=\"hypothesis-data.png\" width='400px' style=\"background-color: white\"/></center>\n",
    "\n",
    "Assume that we have a class of hypothesis $ H $ that uses a function (of multiple functions) for a curve/line to classify the points.\n",
    "Everything above the function is considered red and everything else is blue.\n",
    "\n",
    "Of course we need to learn the exact hypothesis $ h $ we will use from $ H $.\n",
    "Let's assume that we don't split our data, and we learn $ h $ from $ H $ using all of our data.\n",
    "If we do that, then we may end up with a hypothesis represented by the green line.\n",
    "\n",
    "<center><img src=\"hypothesis-bad.png\" width='400px' style=\"background-color: white\"/></center>\n",
    "\n",
    "Technically this is a perfect hypothesis, it scores 100% accuracy.\n",
    "But looking at it, something looks wrong.\n",
    "Even as a junior data scientist / machine learner,\n",
    "this hypothesis should make you feel uneasy.\n",
    "We are generally looking for patterns in our data, but this hypothesis is so specific that it doesn't look like it is representing any general patterns.\n",
    "It looks like it just learned about the specific points in this dataset, and not general patterns.\n",
    "The hypothesis works for these points, but what if we got more points or slightly different points?\n",
    "The hypothesis seems fragile and would probably fail on slightly different points.\n",
    "(This general concept is called [overfitting](https://en.wikipedia.org/wiki/Overfitting),\n",
    "and we will cover it later in this course.)\n",
    "\n",
    "So not only do we have a bad hypothesis/model,\n",
    "but now we can't even figure out how bad it is since we have no data other than the points we gave it.\n",
    "We can't actually evaluate our hypothesis to see how well it does because it already got to see every data point.\n",
    "That's like giving out all the exact questions on a test and then trying to use that exact test again.\n",
    "Later on, we can see this probably in more complex models (especially neural nets) that \"memorize\" data points.\n",
    "In this case, we need data the model has never seen so we can test to see how well it is actually performing.\n",
    "\n",
    "To help solve both of these problems (the model getting to specific and memorizing data points),\n",
    "we can just give our model less data.\n",
    "Specifically, we can \"holdout\" part of our data that the model is not allowed to see when training/fitting,\n",
    "and we can use this data to test/evaluate our model's performance.\n",
    "Using this tactic, we can hopefully train a new hypothesis $ h $ that looks like the line below:\n",
    "\n",
    "<center><img src=\"hypothesis-good.png\" width='400px' style=\"background-color: white\"/></center>\n",
    "\n",
    "Here we can see a much more general model that may miss a few points,\n",
    "but is clearly not over specializing on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973a40c-b504-41f7-ae76-ac78a6e79a99",
   "metadata": {},
   "source": [
    "## Data Splits\n",
    "\n",
    "When we partition our data into multiple parts, we call this \"splitting\" the data,\n",
    "and each partition of the data is called a \"split\".\n",
    "We usually split a dataset into [two or three splits](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets):\n",
    " - Train -- Used to fit (**train** the parameters of) your model/hypothesis.\n",
    " - Test -- Used the test your final model/hypothesis. These are the number that you can report in a paper/publication.\n",
    " - Validation -- Used the train the [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter) of your model. Sometimes left out if you have no hyperparameters.\n",
    "\n",
    "When you have all three splits available, then the proper procedure is to follow the following diagram:\n",
    "\n",
    "<center><img src=\"split-usage.png\" width=\"600px\" style='background-color: white'/></center>\n",
    "\n",
    "We want to make sure that no data from the test split is ever shown to the model during training.\n",
    "We call this [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)).\n",
    "\n",
    "During this course (and many courses), we will take a more relaxed procedure and usually forego the validation data.\n",
    "This makes for an easier workflow where you can replace the validation split in the diagram with our test split.\n",
    "(When publishing results, the diagram must be followed strictly to prevent any data leakage.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f956c-1de8-4eb1-a884-350c1c51e43f",
   "metadata": {},
   "source": [
    "## Creating Splits\n",
    "\n",
    "Creating splits from tabular data is pretty simple.\n",
    "We just need to cut up the list/frame into two or three different chunks.\n",
    "We may also want to shuffle the data before we cut it up.\n",
    "For non-tabular data we will often use [Snowball sampling](https://en.wikipedia.org/wiki/Snowball_sampling),\n",
    "but that is outside the scope of this course.\n",
    "\n",
    "Let's create some test data that we can split!.\n",
    "For this example, we will be generating some fake data using [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).\n",
    "This function is useful for quickly generating some classification data.\n",
    "The data will be pretty simple, but works well as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce1d0c-7e27-42dd-bf2f-9a15ff7619b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 200 -- Make 200 data points.\n",
    "# n_features = 2 -- Generate two feature columns (perfect for plotting).\n",
    "# n_redundant = 0 -- No redundant features (features with the same information as other features).\n",
    "# n_informative = 2 -- Make our two features useful (and not just random).\n",
    "# random_state = 5 -- The seed for the random number generator.\n",
    "#                     The exact number doesn't matter, the same seed will generate the same data.\n",
    "# n_clusters_per_class = 1 -- Make the data simple.\n",
    "all_features, all_labels = sklearn.datasets.make_classification(\n",
    "    n_samples = 200, n_features = 2,\n",
    "    n_redundant = 0, n_informative = 2,\n",
    "    random_state = 5, n_clusters_per_class = 1\n",
    ")\n",
    "\n",
    "# Turn the features into a frame, the labels can stay as a list.\n",
    "all_features = pandas.DataFrame(all_features, columns = ['A', 'B'])\n",
    "\n",
    "print(all_features[0:10])\n",
    "print('---')\n",
    "print(all_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8ac9-8ce8-43fb-8a92-754ae4e6d723",
   "metadata": {},
   "source": [
    "Now that we have some data, we need to split it.\n",
    "\n",
    "We can start really simply and just split is using standard Python operators.\n",
    "Specifically, Python's slicing syntax works well here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06511d-b9cb-4ea8-85de-58d576242ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into even splits (100 train, 100 test).\n",
    "\n",
    "# Take the first 100 points for training.\n",
    "train_features = all_features[:100]\n",
    "train_labels = all_labels[:100]\n",
    "\n",
    "# Take the last 100 points for testing.\n",
    "test_features = all_features[100:]\n",
    "test_labels = all_labels[100:]\n",
    "\n",
    "print(\"Training set size: \", len(train_features))\n",
    "print(train_features[0:5])\n",
    "print(train_labels[0:5])\n",
    "print('---')\n",
    "print(\"Test set size: \", len(test_features))\n",
    "print(test_features[0:5])\n",
    "print(test_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19f815-051d-4bc6-a272-2d397566bc4a",
   "metadata": {},
   "source": [
    "We can also use scikit-learn's builtin function for creating splits,\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c6f5c-7f73-427e-b518-e9767c5d273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = 0.50 -- Use 50% of the data for test. We can also give a fixed number instead of a ratio.\n",
    "# random_state = 4 -- The seed for the random number generator.\n",
    "#                     The exact number doesn't matter, the same seed will generate the same data.\n",
    "#                     By default the data will be shuffled before split, the randomness comes in here.\n",
    "splits = sklearn.model_selection.train_test_split(all_features, all_labels,\n",
    "                                                 test_size = 0.50, random_state = 4)\n",
    "\n",
    "# The return from sklearn.model_selection.train_test_split contains all the split data,\n",
    "# this could have been done on the above line, but it would have made the line very long.\n",
    "train_features, test_features, train_labels, test_labels = splits\n",
    "\n",
    "print(\"Training set size: \", len(train_features))\n",
    "print(train_features[0:5])\n",
    "print(train_labels[0:5])\n",
    "print('---')\n",
    "print(\"Test set size: \", len(test_features))\n",
    "print(test_features[0:5])\n",
    "print(test_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecbb0f-e972-4d65-865f-3c91952c68ce",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "Thus far, we have discussed splitting the once and using the resulting splits.\n",
    "But what if we are unlucky (or lucky depending on how you look at it) and our test set only has easy examples?\n",
    "Or our training set happens to be the best possible training set that we can't expect to see in the real world?\n",
    "To minimize these chances of this happening, we generally like to use multiple sets of data splits.\n",
    "There are many ways multiple splittings of the data can be used,\n",
    "but the most common is probably [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation).\n",
    "\n",
    "In k-fold cross validation, we split out data into k (usually 5 or 10) evenly-sized chunks of data that we call **folds**.\n",
    "One of these folds is assigned to be the test split (also called the **holdout**) and the rest are assigned to be the training split.\n",
    "(You can also use one or more of the folds for a validation set if you need it.)\n",
    "Therefore in 10-fold, 10% of the data is test and 90% is training.\n",
    "After the model is trained on the training split and evaluated on the test spit,\n",
    "the procedure is repeated with the next fold as the test split and the old test fold with the rest of the training data.\n",
    "This is repeated k times (so each fold gets a chance to be the test split) and the results are averaged.\n",
    "\n",
    "<center><img src=\"k-fold.png\" width=\"600px\"/></center>\n",
    "<center style='font-size: small'>Image from <a href='http://karlrosaen.com/ml/learning-log/2016-06-20/'>Karl Rosaen </a>.</center>\n",
    "\n",
    "Not only does this help reduce the randomness of picking good/bad splits,\n",
    "but it also allows us to compute standard deviation and variance on the evaluation scores so we can get an idea how much the data affects our model.\n",
    "\n",
    "Be aware that there are several variations that you can do on k-fold cross validation including (but not limited to):\n",
    " - Using one split for validation.\n",
    " - Instead of splitting your entire dataset, just split the training split and use the original test split as a final evaluation.\n",
    " - Use multiple splits for test data.\n",
    "\n",
    "The concept of k-fold cross-validation is pretty simple,\n",
    "and I am sure you can easily implement your own function to do it.\n",
    "Additionally, you can also use the implementation provided by scikit-learn:\n",
    "[sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html).\n",
    "Note that this function has a lot of parameters and is fairly complex.\n",
    "This is because it can do many of the k-fold variation that we mentioned earlier (as well as other cross-validation methods that are not k-fold).\n",
    "\n",
    "To use scikit-learn's function, you need to have a scikit-learn estimator,\n",
    "which we have not covered yet in this course (we will).\n",
    "But we can still make a simple class to implement a super-simple threshold-based hypothesis that just predicts a label of 1 if the 'A' feature is greater then a threshold\n",
    "(which defaults to 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219130ad-7d18-49b4-aac5-9a0a530318ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scikit-learn estimator that \n",
    "class SuperSimpleHypothesis(sklearn.base.BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.0\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        # Don't do anything.\n",
    "        # We will be using this later, but for now we just want a super simple hypothesis.\n",
    "        pass\n",
    "\n",
    "    def predict(self, features):\n",
    "        labels = []\n",
    "        \n",
    "        for (_, row) in features.iterrows():\n",
    "            label = 0\n",
    "            if (row['A'] > self.threshold):\n",
    "                label = 1\n",
    "\n",
    "            labels.append(label)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbe68a-a505-45d9-b4df-4073e3b0333f",
   "metadata": {},
   "source": [
    "Now that we have a class we can use for our hypothesis, we can do cross-validation on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194cbf5-9790-4581-b982-ab207127f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sklearn.model_selection.cross_validate(SuperSimpleHypothesis(), all_features, all_labels, scoring = 'accuracy')['test_score']\n",
    "\n",
    "print(\"All runs: \", list(scores))\n",
    "print(\"Mean Score: \", numpy.mean(scores), \"Standard Deviation: \", numpy.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c7692-f2a0-4d5f-8d4b-92ec8f307e9b",
   "metadata": {},
   "source": [
    "We can see the scores we got for each of the 5 runs,\n",
    "and we can also compute the mean and standard deviation to get a set of reliable numbers to report the performance of our model.\n",
    "Here we see a decently large range of results, from 92.5% all the way to 100% accuracy.\n",
    "Here we can see the luck in randomized splits playing out.\n",
    "By running over multiple splits, we can get a more realistic view of our model's performance.\n",
    "\n",
    "Note that our hypothesis was very simple and we didn't even bother learning on our training data.\n",
    "In the below class, you can see an example of a hypothesis that learns the threshold from the training data (but it is a bit more complicated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598f5c5-96d0-4aae-95b8-a1630e9fe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scikit-learn estimator that \n",
    "class LearningHypothesis(sklearn.base.BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.0\n",
    "        \n",
    "    def fit(self, features, labels):\n",
    "        \"\"\"\n",
    "        Get the mean of the positive and negative labels,\n",
    "        and put the threshold between those means.\n",
    "        \"\"\"\n",
    "        \n",
    "        zeroes = []\n",
    "        ones = []\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            value = features.iloc[[i]]['A']\n",
    "            \n",
    "            if (label == 0):\n",
    "                zeroes.append(value)\n",
    "            else:\n",
    "                ones.append(value)\n",
    "\n",
    "        self.threshold = ((numpy.mean(zeroes) + numpy.mean(ones)) / 2.0)\n",
    "\n",
    "    def predict(self, features):\n",
    "        labels = []\n",
    "        \n",
    "        for (_, row) in features.iterrows():\n",
    "            label = 0\n",
    "            if (row['A'] > self.threshold):\n",
    "                label = 1\n",
    "\n",
    "            labels.append(label)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4627c5-450d-4127-9520-b918f5502d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sklearn.model_selection.cross_validate(LearningHypothesis(), all_features, all_labels, scoring = 'accuracy')['test_score']\n",
    "\n",
    "print(\"All runs: \", list(scores))\n",
    "print(\"Mean Score: \", numpy.mean(scores), \"Standard Deviation: \", numpy.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7bbe34-d570-423e-8448-25d8d6363d7b",
   "metadata": {},
   "source": [
    "By learning, we can increase our score and decrease our standard deviation (which means we have a more robust hypothesis)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

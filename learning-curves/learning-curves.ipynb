{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503b3143-4be4-4fa9-961f-1bda0d333330",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "\n",
    "TODO\n",
    "We have a predictor (assume a classifier for this example).\n",
    "After we have fit our classifier, how do we know we are done fitting?\n",
    "How do we know that we cannot put more time or data into our classifier to get better results.\n",
    "There are a lot of regorous mathematical answers to these questions,\n",
    "but a simpler and visual way to help answer this question is by using learning curves.\n",
    "\n",
    "TODO\n",
    "A [learning curve](https://en.wikipedia.org/wiki/Learning_curve) is a graph that shows a model's performance as either the amount of training data or time (represented by the number of iterations of some algorithm) increase.\n",
    "You can actually put whatever parameter you want on the x-axis, as long as it represents some sort of ordered progression.\n",
    "Below is an image of a typical learning curve for a hypothetical classifier.\n",
    "\n",
    "TODO\n",
    "IMAGE\n",
    "\n",
    "As we can see in the image above, a typical learning curve will start with a steep increase in performance.\n",
    "The classifier from this image will start by randomly guessing a class label and then just a few pieces of data can start to point it in the right direction.\n",
    "Eventually, we will see the learning rate of the model slow down.\n",
    "This means that the model has seen enough data to do well and any more data is becoming less useful.\n",
    "Finally, we will hit the plateau where more data either will not help at all or only slightly help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e966eb0-303c-41a4-b291-7ea6adaded4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6753f8b-c627-48b7-b13b-936d0af0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TEST\n",
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import sklearn.datasets\n",
    "import sklearn.inspection\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "\n",
    "# TEST\n",
    "\n",
    "# n_samples = 100 -- Make 200 data points.\n",
    "# n_features = 2 -- Generate two feature columns (perfect for plotting decision boundaries).\n",
    "# n_redundant = 0 -- No redundant features (features with the same information as other features).\n",
    "# n_informative = 2 -- Make our two features useful (and not just random).\n",
    "# random_state = 4 -- The seed for the random number generator.\n",
    "#                     The exact number doesn't matter, the same seed will generate the same data.\n",
    "# n_clusters_per_class = 1 -- Make the data simple.\n",
    "features, labels = sklearn.datasets.make_classification(\n",
    "    n_samples = 1000,\n",
    "    n_classes = 2,\n",
    "    n_features = 20,\n",
    "    n_informative = 5,\n",
    "    n_redundant = 15,\n",
    "    n_clusters_per_class = 5,\n",
    "    # n_features = 10,\n",
    "    # n_redundant = 0, n_informative = 2,\n",
    "    random_state = 4,\n",
    "    # n_clusters_per_class = 1\n",
    ")\n",
    "\n",
    "# Turn the features into a frame, the labels can stay as a list.\n",
    "# features = pandas.DataFrame(features, columns = ['A', 'B'])\n",
    "features = pandas.DataFrame(features)\n",
    "\n",
    "''' TEST\n",
    "# Split the data into train and test data.\n",
    "# Note that we are not being rigorous and making sure the splits have the same label breakdown.\n",
    "\n",
    "train_features = all_features[:100]\n",
    "train_labels = all_labels[:100]\n",
    "\n",
    "test_features = all_features[100:]\n",
    "test_labels = all_labels[100:]\n",
    "\n",
    "print(train_features[0:10])\n",
    "print('---')\n",
    "print(train_labels[0:10])\n",
    "'''\n",
    "\n",
    "print(features[0:10])\n",
    "print('---')\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dee99-d90a-40ed-808f-54d2811a90af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22447574-c645-44e4-9213-87fa88395677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# Make a linear regression classifier.\n",
    "# We will discuss the `penalty` argument and why we set it to `None` later.\n",
    "\n",
    "'''\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4,\n",
    "                                                     # penalty = None,\n",
    "                                                     max_iter = 1000000,\n",
    "                                                     solver = 'liblinear',\n",
    "                                                     C = 100000,\n",
    "                                                     )\n",
    "'''\n",
    "\n",
    "classifier = sklearn.tree.DecisionTreeClassifier(random_state = 4,\n",
    "                                                 max_depth = 10,\n",
    "                                                 min_samples_split = 1,\n",
    "                                                 )\n",
    "\n",
    "# figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (6, 6))\n",
    "# axis.set_title(\"Learning Curve for Decision Tree\")\n",
    "\n",
    "# This function will split the data into train/test for us.\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, features, labels,\n",
    "    # cv = sklearn.model_selection.ShuffleSplit(n_splits = 50, test_size = 0.5, random_state = 0),\n",
    "    # train_sizes = [0.01, 0.02, 0.03],\n",
    "    score_type = \"both\",\n",
    "    score_name = \"Accuracy\",\n",
    "    random_state = 4,\n",
    "    \n",
    "    # ax = axis,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065c21d-5b75-4bde-b9ee-334da202aed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cac2f4-5360-44a0-9275-586166d15877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ecbf1-4f24-43f4-95bb-5c6028723de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61da30a5-7da1-4c3b-b019-6d8552b38e9b",
   "metadata": {},
   "source": [
    "Decision trees are notorious for overfitting.\n",
    "They can easily get too complex.\n",
    "\n",
    "Instead of having number of training examples on the x-axis,\n",
    "we are going to have the max tree depth.\n",
    "This is a standin for how complex the model is allowed to be.\n",
    "A deeper tree is generally more complex than shallower one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9da37d-bce6-43d5-a7fe-45856e269eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate decision tree performance on train and test sets with different tree depths\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib import pyplot\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=5, n_redundant=15, random_state=4, n_clusters_per_class = 5)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 41)]\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "\t# configure the model\n",
    "\tmodel = DecisionTreeClassifier(max_depth=i)\n",
    "\t# fit model on the training dataset\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\t# evaluate on the train dataset\n",
    "\ttrain_yhat = model.predict(X_train)\n",
    "\ttrain_acc = accuracy_score(y_train, train_yhat)\n",
    "\ttrain_scores.append(train_acc)\n",
    "\t# evaluate on the test dataset\n",
    "\ttest_yhat = model.predict(X_test)\n",
    "\ttest_acc = accuracy_score(y_test, test_yhat)\n",
    "\ttest_scores.append(test_acc)\n",
    "\t# summarize progress\n",
    "\tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "# plot of train and test scores vs tree depth\n",
    "pyplot.plot(values, train_scores, '-o', label='Train')\n",
    "pyplot.plot(values, test_scores, '-o', label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ec1ca-e3e2-486f-95eb-a5655482ed64",
   "metadata": {},
   "source": [
    "Same type of patter we saw before.\n",
    "Sharp increase, transition, and plateau.\n",
    "Except we also see the plateau getting slightly worse over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb965ddf-77f1-43e2-a03f-fd1a60b6835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587ff36-dafd-42aa-b134-55654d15ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import sklearn.datasets\n",
    "import sklearn.inspection\n",
    "import sklearn.svm\n",
    "\n",
    "FIGURE_SIZE = 5\n",
    "FIGURE_RESOLUTION = 500\n",
    "\n",
    "def visualize_decision_boundary(classifier,\n",
    "                                train_features, train_labels,\n",
    "                                test_features, test_labels,\n",
    "                                title = None):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary of a trained binary classifier\n",
    "    using the FIRST TWO columns of the passed in features.\n",
    "\n",
    "    The train data will be plotted in a lighter shade than the background with a white outline.\n",
    "    The test data will be plotted in a darker shade than the background with a black outline.\n",
    "    \"\"\"\n",
    "\n",
    "    figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (FIGURE_SIZE, FIGURE_SIZE))\n",
    "                                    \n",
    "    matplotlib.pyplot.suptitle(title)\n",
    "\n",
    "    # Score the classifier.\n",
    "    train_accuracy = classifier.score(train_features, train_labels)\n",
    "    test_accuracy = classifier.score(test_features, test_labels)\n",
    "    matplotlib.pyplot.title(\"Train Accuracy: %3.2f, Test Accuracy: %3.2f\" % (train_accuracy,\n",
    "                                                                             test_accuracy))\n",
    "\n",
    "    all_features = pandas.concat([train_features, test_features])\n",
    "    \n",
    "    # TEST\n",
    "    print(all_features.columns)\n",
    "    print(train_features.columns)\n",
    "                                    \n",
    "    # Draw the decision boundary.\n",
    "    sklearn.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, all_features[[all_features.columns[0], all_features.columns[1]]],\n",
    "        response_method = \"predict\", ax = axis,\n",
    "        xlabel = all_features.columns[0], ylabel = all_features.columns[1],\n",
    "        grid_resolution = FIGURE_RESOLUTION,\n",
    "        cmap = 'RdBu', alpha = 0.50,\n",
    "    )\n",
    "\n",
    "    # Display the train data points.\n",
    "    axis.scatter(\n",
    "        train_features[train_features.columns[0]], train_features[train_features.columns[1]],\n",
    "        c = train_labels,\n",
    "        cmap = 'RdBu', alpha = 0.25, edgecolor = 'w',\n",
    "    )\n",
    "\n",
    "    # Display the test data points.\n",
    "    axis.scatter(\n",
    "        test_features[test_features.columns[0]], test_features[test_features.columns[1]],\n",
    "        c = test_labels,\n",
    "        cmap = 'RdBu', alpha = 0.75, edgecolor = 'k',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7736b-2e94-4862-88e3-508203af3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "columns = list(range(0, 20))\n",
    "visualize_decision_boundary(model, pandas.DataFrame(X_train, columns = columns), y_train, pandas.DataFrame(X_test, columns = columns), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729e6ea-0396-4756-ba33-04872fcc2b9c",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

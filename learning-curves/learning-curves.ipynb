{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503b3143-4be4-4fa9-961f-1bda0d333330",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "\n",
    "Let's say that we are creating some classifier.\n",
    "After we have fit our classifier, how do we know we are done fitting?\n",
    "How do we know that we cannot put more time or data into our classifier to get better results?\n",
    "There are a lot of rigorous mathematical answers to these questions,\n",
    "but a simpler and visual way to help answer these questions is by using learning curves.\n",
    "\n",
    "A [learning curve](https://en.wikipedia.org/wiki/Learning_curve) is a graph that shows a model's performance plotted against some parameter about how the model was created/trained.\n",
    "You can actually put whatever parameter you want on the x-axis, as long as it represents some sort of ordered progression like\n",
    "amount of training data, number of iterations, runtime, or model complexity.\n",
    "Together, these two numbers tell a story about how the model is progressing along that parameter\n",
    "(e.g., how the model is progressing as more training data is added).\n",
    "Below is an image of a typical learning curve for a hypothetical classifier that plots accuracy vs fit/training time.\n",
    "\n",
    "<center><img src=\"basic-learning-curve.png\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Image from <a href='https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html'>scikit-learn</a>.</center>\n",
    "\n",
    "As we can see in the image above, a typical learning curve will start with a steep increase in performance.\n",
    "This is because models will generally start from a randomly initialized state and will therefore just guess randomly.\n",
    "Just a few pieces of labeled data will help the classifier do much better than random.\n",
    "Eventually, we will see the improvement rate of the model slow down.\n",
    "This indicates that the model has seen enough data to do well and more data is becoming less useful.\n",
    "Finally, we will hit the plateau where more data either will not help at all or only slightly help.\n",
    "\n",
    "[Looking at the learning curve a model produces](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)\n",
    "can tell us a lot about how it was trained and what we can expect from it going forward.\n",
    "For this example, we will look at some learning curves,\n",
    "discuss the types of patterns we want to see from a learning curve,\n",
    "and see what to do when our learning curves don't look right.\n",
    "\n",
    "Let's start by creating some test data\n",
    "(see the [decision boundary example](https://github.com/ucsc-cse-40/cse40-examples/blob/main/decision-boundaries/decision-boundaries.ipynb) for a discussion on creating toy data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6753f8b-c627-48b7-b13b-936d0af0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "\n",
    "# Make 1000 sample data points.\n",
    "all_features, all_labels = sklearn.datasets.make_classification(\n",
    "    n_samples = 1000,\n",
    "    random_state = 4,\n",
    ")\n",
    "\n",
    "all_features = pandas.DataFrame(all_features)\n",
    "\n",
    "train_features = all_features[:100]\n",
    "train_labels = all_labels[:100]\n",
    "\n",
    "test_features = all_features[100:]\n",
    "test_labels = all_labels[100:]\n",
    "\n",
    "print(all_features[0:10])\n",
    "print('---')\n",
    "print(all_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18c1fb-7f5b-4df1-99b5-4aea63d41958",
   "metadata": {},
   "source": [
    "Now that we have some data, let's just create a simple classifier and look at it's learning curve.\n",
    "To plot learning curves,\n",
    "we will be using [sklearn.model_selection.LearningCurveDisplay.from_estimator()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LearningCurveDisplay.html#sklearn.model_selection.LearningCurveDisplay.from_estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5d40b-8ce4-40f2-ab65-fa8bfd421799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a standard logistic regression classifier.\n",
    "# We make sure to set the seed so the results are always consistent.\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "\n",
    "# Use scikit-learn to show us our learning curve.\n",
    "# This function will split the data into train/test for us,\n",
    "# so we will pass in the full data.\n",
    "# Setting `train_sizes` allows us to get more points on our graph than is standard.\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, all_features, all_labels,\n",
    "    train_sizes = numpy.linspace(0.05, 1.0, 20),\n",
    "    score_name = \"Accuracy\",\n",
    "    random_state = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa1bde-b09f-45bf-a9fe-827eea83c130",
   "metadata": {},
   "source": [
    "Here we can see a pretty standard learning curve for our logistic regression classifier.\n",
    "There is a sharp increase in the beginning, a transition to an area of lower improvement, and a plateau.\n",
    "There are some small blips up and down in the plateau, but overall the accuracy stays about the same.\n",
    "Note that the shaded areas in this graph represent the standard deviation of the accuracy,\n",
    "so you can think of it as the confidence of the graph.\n",
    "\n",
    "This graph is interesting, but when debugging and analyzing classifiers we want to add more to our graph.\n",
    "We are currently displaying the score the classifier gets on the test data,\n",
    "but it is also useful to see the score that the classifier gets on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b070d5-49e1-40e6-99d1-618b675bc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression(random_state = 4)\n",
    "\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, all_features, all_labels,\n",
    "    train_sizes = numpy.linspace(0.05, 1.0, 20),\n",
    "    score_type = \"both\",\n",
    "    score_name = \"Accuracy\",\n",
    "    random_state = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec770ee-dc0f-404c-8913-4cd5f204d70e",
   "metadata": {},
   "source": [
    "Now we can see the score on the test data (in orange) and the score on the training data (in blue) as more labeled data is provided to the classifier.\n",
    "The first question you may be asking is: \"Why does the training score go down as we add more data, is that bad?\".\n",
    "\n",
    "Great question!\n",
    "This behavior is actually not only expected, but desired.\n",
    "Remember that when a model is fitting/learning it only has access to training data, and not test data.\n",
    "So getting a perfect score on training data could mean that the classifier is perfect and will work perfectly on all data (including the test data),\n",
    "or it could mean that the classifier learned about all the quirks in the training data that may not apply to the test data.\n",
    "We call the latter phenomenon [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "As an example of overfitting, think about when you are studying for a test and the professor provides a practice test.\n",
    "A good student will use the practice test to learn about the types of questions and scope of material that will be on the actual test\n",
    "(assuming the practice test is representative of the actual test).\n",
    "This student will (hopefully) perform well when trying the practice test again (like when a classifier is fitting/learning)\n",
    "as well as when taking the actual test (like when a classifier is scoring on the test data).\n",
    "But an overfitting student may learn just the answers to all the questions on the practice test.\n",
    "The overfitting student may get a perfect score when trying the practice test again,\n",
    "but they will surely fail the actual test.\n",
    "\n",
    "To see overfitting in a classification setting, see the data in the blow image.\n",
    "We have two classes, red and blue.\n",
    "We can see that they are roughly split into two groups.\n",
    "There are a few points in either class that are clearly outliers, but the general pattern of the data is straightforward.\n",
    "\n",
    "<center><img src=\"overfitting-data.png\" width='400px' style=\"background-color: white\"/></center>\n",
    "\n",
    "Below, we see a perfectly acceptable decision boundary that generally partitions the data into the two different classes.\n",
    "Of course it will misclassify the outliers, but is respects the general pattern we see in the data.\n",
    "We should expect the classifier that produced this decision boundary to perform well on the test data.\n",
    "\n",
    "<center><img src=\"overfitting-good.png\" width='400px' style=\"background-color: white\"/></center>\n",
    "\n",
    "Below we see an overfit decision boundary.\n",
    "This decision boundary will get a perfect score on the provided training data,\n",
    "but clearly does not respect the general pattern we see in the data.\n",
    "We should expect the classifier that produced this decision boundary to perform poorly on the test data.\n",
    "\n",
    "<center><img src=\"overfitting-bad.png\" width='400px' style=\"background-color: white\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762be31f-5307-4970-9a9e-0f8018ea217b",
   "metadata": {},
   "source": [
    "To see overfitting in-practice,\n",
    "we will use a different classifier that is notorious for overfitting, [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree).\n",
    "At a high-level, decision trees work by creating a tree (the data structure) where each non-leaf node of the tree splits the data according to an attribute and leaf nodes each belong to a class label.\n",
    "If you want more information on decision trees,\n",
    "scikit-learn's [decision tree documentation](https://scikit-learn.org/stable/modules/tree.html) is a good resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22447574-c645-44e4-9213-87fa88395677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic decision tree classifier.\n",
    "classifier = sklearn.tree.DecisionTreeClassifier(random_state = 4)\n",
    "\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, all_features, all_labels,\n",
    "    train_sizes = numpy.linspace(0.05, 1.0, 20),\n",
    "    score_type = \"both\",\n",
    "    score_name = \"Accuracy\",\n",
    "    random_state = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6dc88-ed58-4164-9b8a-1497ae85c0d3",
   "metadata": {},
   "source": [
    "Ouch...\n",
    "The learning curves show that the decision tree classifier always has 100% accuracy on the training data,\n",
    "but ends up around 87.5% accuracy on the test data (where the logistic regression classifier was getting 90% accuracy).\n",
    "\n",
    "Clearly our decision tree classifier is overfitting,\n",
    "but how do we fix it?\n",
    "To start, scikit-learn provides some [tips for dealing with overfitting decision trees](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use).\n",
    "One piece of advice they give is to make the decision tree simpler using the `max_depth` argument (a lower value will force the creation of simpler decision trees).\n",
    "Remember the example with the red and blue dots, more complex models (more squiggly lines) are more likely to overfit,\n",
    "whereas simple models (straighter lines) tend to generalize better and not overfit.\n",
    "\n",
    "But what value of `max_depth` should we use?\n",
    "When will the decision tree become simple enough to not overfit?\n",
    "To answer these questions, we can also use learning curves!\n",
    "But this time instead of plotting number of data points vs score, we will plot the complexity of the model (`max_depth`) vs score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9da37d-bce6-43d5-a7fe-45856e269eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build learning curves with the model complexity (max_depth) on the x-axis.\n",
    "\n",
    "# Go from 1 to 10.\n",
    "max_depths = list(range(1, 11))\n",
    "\n",
    "# Collect the scores for plotting.\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # Make a decision tree classifier with the given complexity (max_depth).\n",
    "    classifier = sklearn.tree.DecisionTreeClassifier(random_state = 4, max_depth = max_depth)\n",
    "\n",
    "    # Fit and score the classifier.\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    train_scores.append(classifier.score(train_features, train_labels))\n",
    "    test_scores.append(classifier.score(test_features, test_labels))\n",
    "    \n",
    "matplotlib.pyplot.plot(max_depths, train_scores, '-o', label = 'Training metric')\n",
    "matplotlib.pyplot.plot(max_depths, test_scores, '-o', label = 'Testing metric')\n",
    "matplotlib.pyplot.legend()\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615a80e-4432-4816-bc5c-e53af6272623",
   "metadata": {},
   "source": [
    "According to these learning curves, the test accuracy starts to decrease when `max_depth` is 3.\n",
    "We also see the training accuracy sharply increase at the same time.\n",
    "So, we can safely assume that this is when our model is starting to overfit.\n",
    "\n",
    "Let's try the decision tree again, but this time with a `max_depth` of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afefe4c-7f7d-4d18-846d-4beea75f13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.tree.DecisionTreeClassifier(random_state = 4, max_depth = 2)\n",
    "\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, all_features, all_labels,\n",
    "    train_sizes = numpy.linspace(0.05, 1.0, 20),\n",
    "    score_type = \"both\",\n",
    "    score_name = \"Accuracy\",\n",
    "    random_state = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d507cea-40e2-453a-b19a-f967c1854f52",
   "metadata": {},
   "source": [
    "Fantastic!\n",
    "Now we see the learning curve behavior that we are looking for.\n",
    "We see a pattern that indicates that our decision tree classifier is now longer overfitting.\n",
    "The classifier ends up scoring about 90% accuracy (actually, 89.50% accuracy) on the test data,\n",
    "which is better than when it was more complex and overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
